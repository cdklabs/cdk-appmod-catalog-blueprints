// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0

import * as path from 'path';
import { PythonFunction } from '@aws-cdk/aws-lambda-python-alpha';
import { Duration, RemovalPolicy, CustomResource, DockerImage } from 'aws-cdk-lib';
import { IVpc, SecurityGroup, Port, ISecurityGroup } from 'aws-cdk-lib/aws-ec2';
import { PolicyStatement, Role, ServicePrincipal, ManagedPolicy, PolicyDocument, Effect } from 'aws-cdk-lib/aws-iam';
import { Architecture, Code, Function as LambdaFunction, Runtime } from 'aws-cdk-lib/aws-lambda';
import { IDatabaseCluster, IDatabaseInstance } from 'aws-cdk-lib/aws-rds';
import { Bucket, BucketEncryption, BlockPublicAccess } from 'aws-cdk-lib/aws-s3';
import { BucketDeployment, Source } from 'aws-cdk-lib/aws-s3-deployment';
import { ISecret } from 'aws-cdk-lib/aws-secretsmanager';
import { StateMachine, DefinitionBody, Map, TaskInput } from 'aws-cdk-lib/aws-stepfunctions';
import { LambdaInvoke } from 'aws-cdk-lib/aws-stepfunctions-tasks';
import { Provider } from 'aws-cdk-lib/custom-resources';
import { Construct } from 'constructs';
import { DefaultRuntimes } from '../framework/custom-resource/default-runtimes';

/**
 * Supported database engines
 */
export enum DatabaseEngine {
  MYSQL = 'mysql',
  POSTGRESQL = 'postgresql',
}

/**
 * Supported file types for data loading
 */
export enum FileType {
  /** Standard SQL file */
  SQL = 'sql',
  /** MySQL dump file generated by mysqldump */
  MYSQLDUMP = 'mysqldump',
  /** PostgreSQL dump file generated by pg_dump */
  PGDUMP = 'pgdump',
}

/**
 * Database connection configuration
 */
export interface DatabaseConfig {
  /** Database engine type */
  readonly engine: DatabaseEngine;
  /** Database cluster (for Aurora) */
  readonly cluster?: IDatabaseCluster;
  /** Database instance (for RDS) */
  readonly instance?: IDatabaseInstance;
  /** Database credentials secret */
  readonly secret: ISecret;
  /** Database name to connect to */
  readonly databaseName: string;
  /** VPC where the database is located */
  readonly vpc: IVpc;
  /** Security group for database access */
  readonly securityGroup: ISecurityGroup;
}

/**
 * File input configuration
 */
export interface FileInput {
  /** Path to the file (local path or S3 URI) */
  readonly filePath: string;
  /** Type of file */
  readonly fileType: FileType;
  /** Execution order (lower numbers execute first) */
  readonly executionOrder?: number;
  /** Whether to continue on error */
  readonly continueOnError?: boolean;
}

/**
 * Properties for the DataLoader construct
 */
export interface DataLoaderProps {
  /** Database configuration */
  readonly databaseConfig: DatabaseConfig;
  /** List of files to load */
  readonly fileInputs: FileInput[];
  /** Optional removal policy for resources (defaults to DESTROY) */
  readonly removalPolicy?: RemovalPolicy;
  /** Optional timeout for Lambda function (defaults to 15 minutes) */
  readonly timeout?: Duration;
  /** Optional memory size for Lambda function (defaults to 1024 MB) */
  readonly memorySize?: number;
}

/**
 * DataLoader construct for loading data into Aurora/RDS databases
 *
 * This construct provides a simplified solution for loading data from various file formats
 * (SQL, mysqldump, pg_dump) into MySQL or PostgreSQL databases. It uses S3 for file storage,
 * Step Functions for orchestration, and Lambda for processing.
 *
 * Architecture:
 * 1. Files are uploaded to S3 bucket
 * 2. Step Function is triggered with list of S3 keys
 * 3. Step Function iterates over files in execution order
 * 4. Lambda function processes each file against the database
 *
 * Example usage:
 * Create a DataLoader with database configuration and file inputs.
 * The construct will handle uploading files to S3, creating a Step Function
 * to orchestrate processing, and executing the data loading pipeline.
 */
export class DataLoader extends Construct {
  /** The S3 bucket used for storing files */
  public readonly bucket: Bucket;
  /** The Step Functions state machine for orchestration */
  public readonly stateMachine: StateMachine;
  /** The Lambda function that processes the data loading */
  public readonly processorFunction: LambdaFunction;
  /** The bucket deployment for uploading files */
  public bucketDeployment?: BucketDeployment;
  /** The custom resource provider for triggering state machine execution */
  public readonly customResourceProvider: Provider;
  /** The custom resource that triggers the state machine */
  public readonly executionTrigger: CustomResource;
  /** The file inputs configuration */
  private readonly fileInputs: FileInput[];

  constructor(scope: Construct, id: string, props: DataLoaderProps) {
    super(scope, id);

    // Store file inputs for later use
    this.fileInputs = props.fileInputs;

    // Validate props
    this._validateProps(props);

    // Get removal policy with default
    const removalPolicy = props.removalPolicy || RemovalPolicy.DESTROY;

    // Create S3 bucket for storing files
    this.bucket = this._createBucket(removalPolicy);

    // Create Lambda function for processing
    this.processorFunction = this._createProcessorFunction(props);

    // Create Step Functions state machine
    this.stateMachine = this._createStateMachine();

    // Create custom resource provider for triggering execution
    this.customResourceProvider = this._createCustomResourceProvider();

    // Upload files to S3
    this._setupFileProcessing(props);

    // Create custom resource to trigger execution after files are uploaded
    this.executionTrigger = this._createExecutionTrigger();
  }

  /**
   * Grants additional IAM permissions to the execution trigger Lambda function
   * @param statement The IAM policy statement to add
   */
  public grantExecutionTriggerPermissions(statement: PolicyStatement): void {
    // Get the Lambda function from the custom resource provider
    const triggerFunction = this.customResourceProvider.onEventHandler;
    if (triggerFunction) {
      triggerFunction.addToRolePolicy(statement);
    }
  }

  /**
   * Validates the construct properties
   * @param props The DataLoader properties
   * @private
   */
  private _validateProps(props: DataLoaderProps): void {
    if (!props.databaseConfig) {
      throw new Error('databaseConfig is required');
    }

    if (!props.databaseConfig.cluster && !props.databaseConfig.instance) {
      throw new Error('Either cluster or instance must be provided in databaseConfig');
    }

    if (!props.fileInputs || props.fileInputs.length === 0) {
      throw new Error('At least one file input is required');
    }

    // Validate file inputs
    for (const fileInput of props.fileInputs) {
      if (!fileInput.filePath) {
        throw new Error('filePath is required for each file input');
      }
      if (!fileInput.fileType) {
        throw new Error('fileType is required for each file input');
      }
    }

    // Validate engine compatibility
    for (const fileInput of props.fileInputs) {
      if (props.databaseConfig.engine === DatabaseEngine.MYSQL &&
          fileInput.fileType === FileType.PGDUMP) {
        throw new Error('PostgreSQL dump files cannot be used with MySQL databases');
      }
      if (props.databaseConfig.engine === DatabaseEngine.POSTGRESQL &&
          fileInput.fileType === FileType.MYSQLDUMP) {
        throw new Error('MySQL dump files cannot be used with PostgreSQL databases');
      }
    }
  }

  /**
   * Creates the S3 bucket for storing files
   * @param removalPolicy The removal policy to apply
   * @returns The created S3 bucket
   * @private
   */
  private _createBucket(removalPolicy: RemovalPolicy): Bucket {
    const bucket = new Bucket(this, 'DataLoaderBucket', {
      encryption: BucketEncryption.S3_MANAGED,
      blockPublicAccess: BlockPublicAccess.BLOCK_ALL,
      removalPolicy: removalPolicy,
      autoDeleteObjects: removalPolicy === RemovalPolicy.DESTROY,
    });

    return bucket;
  }

  /**
   * Creates the Lambda function for processing data loading
   * @param props The DataLoader properties
   * @returns The created Lambda function
   * @private
   */
  private _createProcessorFunction(props: DataLoaderProps): LambdaFunction {
    // Create a dedicated security group for the Lambda function
    const lambdaSecurityGroup = new SecurityGroup(this, 'DataLoaderProcessorSecurityGroup', {
      vpc: props.databaseConfig.vpc,
      description: 'Security group for DataLoader processor Lambda function',
      allowAllOutbound: true, // Lambda needs outbound access for AWS services and internet
    });

    // Allow Lambda to connect to the database
    // Add ingress rule to database security group to allow connections from Lambda
    props.databaseConfig.securityGroup.addIngressRule(
      lambdaSecurityGroup,
      Port.tcp(props.databaseConfig.engine === DatabaseEngine.MYSQL ? 3306 : 5432),
      `Allow DataLoader Lambda to connect to ${props.databaseConfig.engine} database`,
    );

    // Create Lambda function with automatic dependency bundling
    const lambdaFunction = new PythonFunction(this, 'DataLoaderProcessor', {
      entry: path.join(__dirname, 'data-loader-lambda'),
      runtime: Runtime.PYTHON_3_13,
      handler: 'handler',
      index: 'index.py',
      timeout: props.timeout || Duration.minutes(15),
      memorySize: props.memorySize || 1024,
      architecture: Architecture.ARM_64,
      vpc: props.databaseConfig.vpc,
      securityGroups: [lambdaSecurityGroup], // Use the dedicated Lambda security group
      environment: {
        DATABASE_ENGINE: props.databaseConfig.engine,
        DATABASE_SECRET_ARN: props.databaseConfig.secret.secretArn,
        DATABASE_NAME: props.databaseConfig.databaseName,
        S3_BUCKET: this.bucket.bucketName,
      },
      bundling: {
        // Use custom bundling commands to avoid Poetry conflicts
        command: [
          'bash', '-c', [
            // Create a clean virtual environment
            'python -m venv /tmp/venv',
            // Activate the virtual environment
            'source /tmp/venv/bin/activate',
            // Install dependencies in the virtual environment
            'pip install --upgrade pip',
            'pip install -r requirements.txt -t /asset-output',
            // Copy source files
            'cp -r . /asset-output',
            // Clean up __pycache__ and other unnecessary files
            'find /asset-output -type d -name __pycache__ -exec rm -rf {} + || true',
            'find /asset-output -name "*.pyc" -delete || true',
          ].join(' && '),
        ],
        // Use the standard Python image to avoid pre-installed packages
        image: DockerImage.fromRegistry(DefaultRuntimes.PYTHON_BUNDLING_IMAGE),
      },
    });

    // Grant permissions
    this._grantPermissions(lambdaFunction, props);

    return lambdaFunction;
  }

  /**
   * Creates the Step Functions state machine
   * @returns The created state machine
   * @private
   */
  private _createStateMachine(): StateMachine {
    // Create a Map state to iterate over file keys
    const processFileTask = new LambdaInvoke(this, 'ProcessFileTask', {
      lambdaFunction: this.processorFunction,
      payload: TaskInput.fromObject({
        's3Key.$': '$',
        'bucketName': this.bucket.bucketName,
      }),
    });

    // Create the state machine definition
    const definition = new Map(this, 'ProcessFilesMap', {
      itemsPath: '$.fileKeys',
      maxConcurrency: 1, // Process files sequentially to maintain order
    }).itemProcessor(processFileTask);

    // Create the state machine
    const stateMachine = new StateMachine(this, 'DataLoaderStateMachine', {
      definitionBody: DefinitionBody.fromChainable(definition),
      timeout: Duration.hours(2), // Allow up to 2 hours for the entire process
    });

    // Grant the state machine permission to invoke the Lambda function
    this.processorFunction.grantInvoke(stateMachine);

    return stateMachine;
  }

  /**
   * Grants necessary permissions to the Lambda function
   * @param lambdaFunction The Lambda function
   * @param props The DataLoader properties
   * @private
   */
  private _grantPermissions(lambdaFunction: LambdaFunction, props: DataLoaderProps): void {
    // Grant S3 permissions
    this.bucket.grantRead(lambdaFunction);

    // Grant Secrets Manager permissions
    props.databaseConfig.secret.grantRead(lambdaFunction);
  }

  /**
   * Sets up file processing by uploading files to S3
   * @param props The DataLoader properties
   * @private
   */
  private _setupFileProcessing(props: DataLoaderProps): void {
    // Separate local files from S3 URIs
    const localFiles = props.fileInputs.filter(f => !f.filePath.startsWith('s3://'));

    // Upload local files to S3 if any
    if (localFiles.length > 0) {
      // Process each file individually to handle both files and directories
      localFiles.forEach((fileInput, index) => {
        const filePath = fileInput.filePath;
        // eslint-disable-next-line @typescript-eslint/no-require-imports
        const fs = require('fs');
        // eslint-disable-next-line @typescript-eslint/no-require-imports
        const pathModule = require('path');

        let resolvedPath: string;
        if (pathModule.isAbsolute(filePath)) {
          resolvedPath = filePath;
        } else {
          // Resolve relative paths from the current working directory
          resolvedPath = pathModule.resolve(filePath);
        }

        // Check if the path exists
        if (!fs.existsSync(resolvedPath)) {
          throw new Error(`File not found: ${resolvedPath}`);
        }

        if (fs.statSync(resolvedPath).isFile()) {
          // For individual files, deploy from parent directory with include filter
          const parentDir = pathModule.dirname(resolvedPath);
          const fileName = pathModule.basename(resolvedPath);

          new BucketDeployment(this, `DataLoaderFileDeployment${index}`, {
            sources: [Source.asset(parentDir)],
            destinationBucket: this.bucket,
            destinationKeyPrefix: 'data-files/',
            include: [fileName],
          });
        } else {
          // For directories, deploy entire directory
          new BucketDeployment(this, `DataLoaderFileDeployment${index}`, {
            sources: [Source.asset(resolvedPath)],
            destinationBucket: this.bucket,
            destinationKeyPrefix: 'data-files/',
          });
        }
      });
    }
  }

  /**
   * Creates a custom resource provider for triggering state machine execution
   * @returns The custom resource provider
   * @private
   */
  private _createCustomResourceProvider(): Provider {
    // Create IAM role for the custom resource Lambda function with all necessary policies
    const customResourceRole = new Role(this, 'StateMachineExecutionTriggerRole', {
      assumedBy: new ServicePrincipal('lambda.amazonaws.com'),
      description: 'IAM role for DataLoader custom resource Lambda function',
      managedPolicies: [
        ManagedPolicy.fromAwsManagedPolicyName('service-role/AWSLambdaBasicExecutionRole'),
      ],
      inlinePolicies: {
        StateMachineExecutionPolicy: new PolicyDocument({
          statements: [
            new PolicyStatement({
              effect: Effect.ALLOW,
              actions: [
                'states:StartExecution',
                'states:DescribeExecution',
                'states:StopExecution',
              ],
              resources: [
                this.stateMachine.stateMachineArn,
                `${this.stateMachine.stateMachineArn}:*`, // For execution ARNs
              ],
            }),
          ],
        }),
      },
    });

    // Create Lambda function for custom resource with the pre-configured role
    const customResourceFunction = new LambdaFunction(this, 'StateMachineExecutionTrigger', {
      runtime: Runtime.PYTHON_3_11,
      handler: 'index.handler',
      role: customResourceRole, // Attach the role during creation
      code: Code.fromInline(`
import json
import boto3
import logging

logger = logging.getLogger()
logger.setLevel(logging.INFO)

stepfunctions = boto3.client('stepfunctions')

def handler(event, context):
    logger.info(f"Received event: {json.dumps(event)}")
    
    request_type = event['RequestType']
    
    if request_type == 'Create' or request_type == 'Update':
        try:
            # Get parameters from event
            state_machine_arn = event['ResourceProperties']['StateMachineArn']
            file_keys = event['ResourceProperties']['FileKeys']
            
            # Start execution
            response = stepfunctions.start_execution(
                stateMachineArn=state_machine_arn,
                input=json.dumps({
                    'fileKeys': file_keys
                })
            )
            
            execution_arn = response['executionArn']
            logger.info(f"Started execution: {execution_arn}")
            
            return {
                'Status': 'SUCCESS',
                'PhysicalResourceId': execution_arn,
                'Data': {
                    'ExecutionArn': execution_arn
                }
            }
            
        except Exception as e:
            logger.error(f"Error starting execution: {str(e)}")
            return {
                'Status': 'FAILED',
                'Reason': str(e),
                'PhysicalResourceId': 'failed'
            }
    
    elif request_type == 'Delete':
        # For delete, we don't need to do anything special
        return {
            'Status': 'SUCCESS',
            'PhysicalResourceId': event.get('PhysicalResourceId', 'none')
        }
    
    return {
        'Status': 'SUCCESS',
        'PhysicalResourceId': 'none'
    }
      `),
      timeout: Duration.minutes(5),
    });

    // Create provider with the Lambda function that already has the correct role
    const provider = new Provider(this, 'StateMachineExecutionProvider', {
      onEventHandler: customResourceFunction,
    });

    return provider;
  }

  /**
   * Creates a custom resource to trigger state machine execution
   * @returns The custom resource
   * @private
   */
  private _createExecutionTrigger(): CustomResource {
    // Get ordered file keys
    const orderedFileKeys = this._getOrderedFileKeys();

    const customResource = new CustomResource(this, 'ExecutionTriggerResource', {
      serviceToken: this.customResourceProvider.serviceToken,
      properties: {
        StateMachineArn: this.stateMachine.stateMachineArn,
        FileKeys: orderedFileKeys,
        // Add a timestamp to force updates when needed
        Timestamp: Date.now().toString(),
      },
    });

    // Ensure the custom resource runs after bucket deployment (if exists)
    if (this.bucketDeployment) {
      customResource.node.addDependency(this.stateMachine);
      customResource.node.addDependency(this.bucketDeployment);
    }

    return customResource;
  }

  /**
   * Gets the ordered file keys for execution
   * @returns Array of S3 keys in execution order
   * @private
   */
  private _getOrderedFileKeys(): string[] {
    // Sort files by execution order
    const sortedFiles = [...this.fileInputs].sort(
      (a, b) => (a.executionOrder || 0) - (b.executionOrder || 0),
    );

    // Convert file paths to S3 keys
    return sortedFiles.map(file => {
      if (file.filePath.startsWith('s3://')) {
        // Extract key from S3 URI
        const parts = file.filePath.replace('s3://', '').split('/');
        return parts.slice(1).join('/'); // Remove bucket name
      } else {
        // Local files are uploaded with data-files/ prefix
        const fileName = path.basename(file.filePath);
        return `data-files/${fileName}`;
      }
    });
  }
}
